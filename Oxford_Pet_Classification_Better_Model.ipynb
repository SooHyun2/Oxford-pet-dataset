{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07.Oxford_Pet_Classification_Better_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SooHyun2/Oxford-pet-dataset/blob/main/Oxford_Pet_Classification_Better_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOlR5SRQ_rRn"
      },
      "source": [
        "## library import\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q3wx3rfAKD9"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INMNWadsDycA"
      },
      "source": [
        "## Data 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu7sQGmLANhW"
      },
      "source": [
        "## google drive에서 압축된 dataset download\n",
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1dIR9ANjUsV9dWa0pS9J0c2KUGMfpIRG0'\n",
        "fname = 'oxford_pet.zip'\n",
        "gdown.download(url, fname, quiet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDkv_v3rASc2"
      },
      "source": [
        "## oxford_pet.zip이 보이는지 확인\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdYtua7-AZw2"
      },
      "source": [
        "## 압축풀기\n",
        "!unzip -q oxford_pet.zip -d oxford_pet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWlYo19mAckW"
      },
      "source": [
        "## 압축이 풀린 directory 확인\n",
        "!ls oxford_pet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUP8ce30AhSe"
      },
      "source": [
        "## directory 설정\n",
        "cur_dir = os.getcwd()\n",
        "data_dir = os.path.join(cur_dir, 'oxford_pet')\n",
        "image_dir = os.path.join(data_dir, 'images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN8xhsaAAjj2"
      },
      "source": [
        "## image file 수 확인\n",
        "image_files = [fname for fname in os.listdir(image_dir) if os.path.splitext(fname)[-1] == '.jpg']\n",
        "print(len(image_files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTDYdL5dAlCm"
      },
      "source": [
        "## image file들을 읽어서 channel이 3이 아닌 image는 삭제\n",
        "for image_file in image_files:\n",
        "  image_path = os.path.join(image_dir, image_file)\n",
        "  image = Image.open(image_path)\n",
        "  image_mode = image.mode\n",
        "  if image_mode != 'RGB':\n",
        "    print(image_file, image_mode)\n",
        "    image = np.asarray(image)\n",
        "    print(image.shape)\n",
        "    os.remove(image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cppg8h2VAw4Z"
      },
      "source": [
        "## image file 수 확인\n",
        "image_files = [fname for fname in os.listdir(image_dir) if os.path.splitext(fname)[-1] == '.jpg']\n",
        "print(len(image_files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "222XfHgfBEn8"
      },
      "source": [
        "class_list = set()\n",
        "for image_file in image_files:\n",
        "    file_name = os.path.splitext(image_file)[0]\n",
        "    class_name = re.sub('_\\d+', '', file_name)\n",
        "    class_list.add(class_name)\n",
        "class_list = list(class_list)\n",
        "print(len(class_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgAn11sPBoCk"
      },
      "source": [
        "class_list.sort()\n",
        "class_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSR0PnMuB-RB"
      },
      "source": [
        "class_list[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSD0OPZiBqcW"
      },
      "source": [
        "class2idx = {cls:idx for idx, cls in enumerate(class_list)}\n",
        "class2idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28uLAxq7B28m"
      },
      "source": [
        "class2idx['Bengal']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYVLpAqbCDVD"
      },
      "source": [
        "## train, validation directory 생성\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "val_dir = os.path.join(data_dir, 'validation')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BvTqU94CNDI"
      },
      "source": [
        "image_files.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bWy45HtCRfu"
      },
      "source": [
        "image_files[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vtk6sPRCSmG"
      },
      "source": [
        "cnt = 0\n",
        "previous_class = \"\"\n",
        "for image_file in image_files:\n",
        "    file_name = os.path.splitext(image_file)[0]\n",
        "    class_name = re.sub('_\\d+', '', file_name)\n",
        "    if class_name == previous_class:\n",
        "        cnt += 1\n",
        "    else:\n",
        "        cnt = 1\n",
        "    if cnt <= 160:\n",
        "        cpath = train_dir\n",
        "    else:\n",
        "        cpath = val_dir\n",
        "    image_path = os.path.join(image_dir, image_file)\n",
        "    shutil.copy(image_path, cpath)\n",
        "    previous_class = class_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBFCxHS4DUPZ"
      },
      "source": [
        "train_images = os.listdir(train_dir)\n",
        "val_images = os.listdir(val_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QozFeSbWDXtW"
      },
      "source": [
        "print(len(train_images), len(val_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkE29AuyDZke"
      },
      "source": [
        "train_images[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWM5lj9-DbKz"
      },
      "source": [
        "val_images[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k346RhAjDtZp"
      },
      "source": [
        "## TFRecord File 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69VyTmqqDc2b"
      },
      "source": [
        "IMG_SIZE = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa4QauQ7DsCp"
      },
      "source": [
        "## TFRecord 저장할 directory와 file 경로 설정\n",
        "tfr_dir = os.path.join(data_dir, 'tfrecord')\n",
        "os.makedirs(tfr_dir, exist_ok=True)\n",
        "\n",
        "tfr_train_dir = os.path.join(tfr_dir, 'cls_train.tfr')\n",
        "tfr_val_dir = os.path.join(tfr_dir, 'cls_val.tfr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGRR9-QLIJ6Q"
      },
      "source": [
        "## TFRecord writer 생성\n",
        "writer_train = tf.io.TFRecordWriter(tfr_train_dir)\n",
        "writer_val = tf.io.TFRecordWriter(tfr_val_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9C7LmF3Fl4Q"
      },
      "source": [
        "# The following functions can be used to convert a value to a type compatible\n",
        "# with tf.Example.\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "def _int64_feature(value):\n",
        "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrDR5d2SGZBY"
      },
      "source": [
        "n_train = 0\n",
        "\n",
        "train_files = os.listdir(train_dir)\n",
        "for train_file in train_files:\n",
        "  train_path = os.path.join(train_dir, train_file)\n",
        "  image = Image.open(train_path)\n",
        "  image = image.resize((IMG_SIZE, IMG_SIZE))\n",
        "  bimage = image.tobytes()\n",
        "\n",
        "  file_name = os.path.splitext(train_file)[0] #Bangal_101\n",
        "  class_name = re.sub('_\\d+', '', file_name)\n",
        "  class_num = class2idx[class_name]\n",
        "\n",
        "  example = tf.train.Example(features=tf.train.Features(feature={\n",
        "      'image': _bytes_feature(bimage),\n",
        "      'cls_num': _int64_feature(class_num)\n",
        "  }))\n",
        "  writer_train.write(example.SerializeToString())\n",
        "  n_train += 1\n",
        "\n",
        "writer_train.close()\n",
        "print(n_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0HL5fAeHm7Q"
      },
      "source": [
        "n_val = 0\n",
        "\n",
        "val_files = os.listdir(val_dir)\n",
        "for val_file in val_files:\n",
        "  val_path = os.path.join(val_dir, val_file)\n",
        "  image = Image.open(val_path)\n",
        "  image = image.resize((IMG_SIZE, IMG_SIZE))\n",
        "  bimage = image.tobytes()\n",
        "\n",
        "  file_name = os.path.splitext(val_file)[0] #Bangal_101\n",
        "  class_name = re.sub('_\\d+', '', file_name)\n",
        "  class_num = class2idx[class_name]\n",
        "\n",
        "  example = tf.train.Example(features=tf.train.Features(feature={\n",
        "      'image': _bytes_feature(bimage),\n",
        "      'cls_num': _int64_feature(class_num)\n",
        "  }))\n",
        "  writer_val.write(example.SerializeToString())\n",
        "  n_val += 1\n",
        "\n",
        "writer_val.close()\n",
        "print(n_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1JBx-YMH8iP"
      },
      "source": [
        "!ls -l $tfr_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9l6lGpGJA0a"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q67RpcOWIHk3"
      },
      "source": [
        "## Hyper Parameters\n",
        "N_CLASS = len(class_list)\n",
        "N_EPOCHS = 50\n",
        "N_BATCH = 40\n",
        "N_TRAIN = n_train\n",
        "N_VAL = n_val\n",
        "IMG_SIZE = 224\n",
        "learning_rate = 0.0001\n",
        "steps_per_epoch = N_TRAIN / N_BATCH\n",
        "validation_steps = int(np.ceil(N_VAL / N_BATCH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hci5uW4kUwcm"
      },
      "source": [
        "## tfrecord file을 data로 parsing해주는 function\n",
        "def _parse_function(tfrecord_serialized):\n",
        "    features={'image': tf.io.FixedLenFeature([], tf.string),\n",
        "              'cls_num': tf.io.FixedLenFeature([], tf.int64)\n",
        "             }\n",
        "    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)\n",
        "    \n",
        "    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
        "    image = tf.reshape(image, [IMG_SIZE, IMG_SIZE, 3])\n",
        "    image = tf.cast(image, tf.float32)/255. \n",
        "\n",
        "    label = tf.cast(parsed_features['cls_num'], tf.int64)\n",
        "    label = tf.one_hot(label, N_CLASS)\n",
        "\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATjjpM0YZLQ-"
      },
      "source": [
        "## CutMix\n",
        "def cutmix(images, labels, PROB=0.5):  \n",
        "  imgs = []; labs = []\n",
        "  for i in range(N_BATCH):\n",
        "    APPLY = tf.cast(tf.random.uniform(()) <= PROB, tf.int32)\n",
        "    idx = tf.random.uniform((), 0, N_BATCH, tf.int32)\n",
        "\n",
        "    W = IMG_SIZE; H = IMG_SIZE\n",
        "    lam = tf.random.uniform(())\n",
        "    cut_ratio = tf.math.sqrt(1.-lam)    \n",
        "    cut_w = tf.cast(W * cut_ratio, tf.int32) * APPLY\n",
        "    cut_h = tf.cast(H * cut_ratio, tf.int32) * APPLY\n",
        "\n",
        "    cx = tf.random.uniform((), int(W/8), int(7/8*W), tf.int32)\n",
        "    cy = tf.random.uniform((), int(H/8), int(7/8*H), tf.int32)\n",
        "\n",
        "    xmin = tf.clip_by_value(cx - cut_w//2, 0, W)\n",
        "    ymin = tf.clip_by_value(cy - cut_h//2, 0, H)\n",
        "    xmax = tf.clip_by_value(cx + cut_w//2, 0, W)\n",
        "    ymax = tf.clip_by_value(cy + cut_h//2, 0, H)    \n",
        "    \n",
        "    mid_left = images[i, ymin:ymax, :xmin, :]\n",
        "    mid_mid = images[idx, ymin:ymax, xmin:xmax, :]\n",
        "    mid_right = images[i, ymin:ymax, xmax:, :]\n",
        "    middle = tf.concat([mid_left, mid_mid, mid_right], axis=1)\n",
        "    top = images[i, :ymin, :, :]\n",
        "    bottom = images[i, ymax:, :, :]\n",
        "    new_img = tf.concat([top, middle, bottom], axis=0)\n",
        "    imgs.append(new_img)\n",
        "    \n",
        "    alpha = tf.cast((cut_w*cut_h)/(W*H), tf.float32)\n",
        "    label1 = labels[i]; label2 = labels[idx]\n",
        "    new_label = ((1-alpha)*label1 + alpha*label2)\n",
        "    labs.append(new_label)\n",
        "\n",
        "  new_imgs = tf.reshape(tf.stack(imgs), [-1, IMG_SIZE, IMG_SIZE, 3])\n",
        "  new_labs = tf.reshape(tf.stack(labs), [-1, N_CLASS])\n",
        "\n",
        "  return new_imgs, new_labs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH4Qz4TVVmBN"
      },
      "source": [
        "## train dataset 만들기\n",
        "train_dataset = tf.data.TFRecordDataset(tfr_train_dir)\n",
        "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=N_TRAIN).prefetch(\n",
        "    tf.data.experimental.AUTOTUNE).batch(N_BATCH)\n",
        "train_dataset = train_dataset.map(cutmix).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uvaztzXVmBV"
      },
      "source": [
        "## validation dataset 만들기\n",
        "val_dataset = tf.data.TFRecordDataset(tfr_val_dir)\n",
        "val_dataset = val_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(N_BATCH).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bzpy01e_AF3"
      },
      "source": [
        "for image, label in train_dataset.take(1):\n",
        "  for i in range(N_BATCH):    \n",
        "    print(label[i].numpy())\n",
        "    plt.imshow(image[i])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10S3kimgMLsa"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.layers import Conv2D, ReLU, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLX1lM98Q_Xo"
      },
      "source": [
        "mobilenetv2 = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfiuca4mIugp"
      },
      "source": [
        "def create_mv_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(mobilenetv2)\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "  model.add(Dense(256))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(ReLU())\n",
        "  model.add(Dense(N_CLASS, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HtGGlpK1Mmy"
      },
      "source": [
        "## Create model, compile & summary\n",
        "model = create_mv_model()\n",
        "\n",
        "LR_INIT = 0.000001\n",
        "LR_MAX = 0.0002\n",
        "LR_MIN = LR_INIT\n",
        "RAMPUP_EPOCH = 4\n",
        "EXP_DECAY = 0.9\n",
        "\n",
        "def lr_schedule_fn(epoch):\n",
        "  if epoch < RAMPUP_EPOCH:\n",
        "    lr = (LR_MAX - LR_MIN) / RAMPUP_EPOCH * epoch + LR_INIT\n",
        "  else:\n",
        "    lr = (LR_MAX - LR_MIN) * EXP_DECAY**(epoch - RAMPUP_EPOCH)\n",
        "  return lr\n",
        "\n",
        "lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule_fn)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(LR_INIT),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45ZUYyvkUQ8N"
      },
      "source": [
        "def plot_lr():\n",
        "  lr = []\n",
        "  epoch_list = list(np.arange(N_EPOCHS) + 1)\n",
        "  for epoch in range(N_EPOCHS):\n",
        "    lr.append(lr_schedule_fn(epoch))    \n",
        "  plt.plot(epoch_list, lr, 'b-')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.show()\n",
        "\n",
        "plot_lr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVG-WgmoH5OG"
      },
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=N_EPOCHS,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[lr_callback]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imob7MgmOJON"
      },
      "source": [
        "from tensorflow.keras.applications.densenet import DenseNet121"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UuHEHwzPPPi"
      },
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.layers import Conv2D, ReLU, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKRJ_-jlPPPp"
      },
      "source": [
        "densenet = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWc262UnPf1l"
      },
      "source": [
        "densenet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMdX9m0XOPee"
      },
      "source": [
        "def create_dense_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(densenet)\n",
        "  model.add(GlobalAveragePooling2D())\n",
        "  model.add(Dense(256))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(ReLU())\n",
        "  model.add(Dense(N_CLASS, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX_TnX2YOQwU"
      },
      "source": [
        "## Create model, compile & summary\n",
        "model = create_dense_model()\n",
        "\n",
        "LR_INIT = 0.000001\n",
        "LR_MAX = 0.0002\n",
        "LR_MIN = LR_INIT\n",
        "RAMPUP_EPOCH = 4\n",
        "EXP_DECAY = 0.9\n",
        "\n",
        "def lr_schedule_fn(epoch):\n",
        "  if epoch < RAMPUP_EPOCH:\n",
        "    lr = (LR_MAX - LR_MIN) / RAMPUP_EPOCH * epoch + LR_INIT\n",
        "  else:\n",
        "    lr = (LR_MAX - LR_MIN) * EXP_DECAY**(epoch - RAMPUP_EPOCH)\n",
        "  return lr\n",
        "\n",
        "lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule_fn)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(LR_INIT),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh5d6-QpPpuV"
      },
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=N_EPOCHS,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[lr_callback]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql0km5gcju-1"
      },
      "source": [
        "## 새로운 Image로 Test하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O2w2dvJjWuX"
      },
      "source": [
        "## Image upload 후 실행\n",
        "image = Image.open('xxx.jpg')\n",
        "image = image.resize((224, 224))\n",
        "image = np.array(image)\n",
        "image = image/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agwPFMDoVO2G"
      },
      "source": [
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpYo45WUj1o3"
      },
      "source": [
        "image = np.reshape(image, (1, 224, 224, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HycF7bJQj1vK"
      },
      "source": [
        "prediction = model.predict(image)\n",
        "prediction.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoGAic17j1sT"
      },
      "source": [
        "pred_class = np.argmax(prediction, axis=-1)\n",
        "pred_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jdc6_V0kQBY"
      },
      "source": [
        "class_list[int(pred_class)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwHc4KE0kUvV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}